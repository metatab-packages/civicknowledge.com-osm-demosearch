{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "architectural-luther",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Open Street Map Extract for Demographic Search</h1>\n",
       "<p><code>civicknowledge.com-osm-demosearch-1.1.3</code> Last Update: 2021-02-06T17:04:58</p>\n",
       "<p>__</p>\n",
       "<h2>Contacts</h2>\n",
       "<ul>\n",
       "<li><strong>Wrangler</strong> <a href=\"mailto:eric@civicknowledge.com\">Eric Busboom</a>, <a href=\"http://civicknowledge.com\">Civic Knowledge</a></li>\n",
       "</ul>\n",
       "<h2>Resources</h2>\n",
       "<ul>\n",
       "<li><strong> <a href=\"data/geohash_tags.csv\">geohash_tags</a></strong>. Points converted to counts of tags per geohash</li>\n",
       "<li><strong> <a href=\"data/residential_roads.csv\">residential_roads</a></strong>. Residential roads per 4 digit geohash</li>\n",
       "<li><strong> <a href=\"data/nonres_roads.csv\">nonres_roads</a></strong>. Non residential roads per 4 digit geohash</li>\n",
       "</ul>\n",
       "<h2>References</h2>\n",
       "<ul><li> <strong>us_geohashes</string>, <em>metapack+http://library.metatab.org/civicknowledge.com-geohash-us.csv#us_geohashes</em>. All 4 digit geohases in the continential US</li><li> <strong><a href=\"https://download.geofabrik.de/north-america-latest.osm.pbf\">north-america-latest</a></strong>. OSM North America extract</li><li> <strong>points</string>, <em>data/csv/points.csv</em>. Points from the OSM file</li><li> <strong>lines</string>, <em>data/csv/lines.csv</em>. Lines from the OSM file</li><li> <strong>multipolygons</string>, <em>data/csv/multipolygons.csv</em>. Polygons from the OSM file</li><li> <strong>multilinestrings</string>, <em>data/csv/multilinestrings.csv</em>. Lines from the OSM file</li><li> <strong>other_relations</string>, <em>data/csv/other_relations.csv</em>. Other geo data from the OSM file</li><ul>"
      ],
      "text/plain": [
       "# Open Street Map Extract for Demographic Search\n",
       "`civicknowledge.com-osm-demosearch-1.1.3` Last Update: 2021-02-06T17:04:58\n",
       "\n",
       "__\n",
       "\n",
       "\n",
       "\n",
       "\n",
       " \n",
       "\n",
       "## Contacts\n",
       "\n",
       "* **Wrangler** [Eric Busboom](mailto:eric@civicknowledge.com), [Civic Knowledge](http://civicknowledge.com)\n",
       "\n",
       "## Resources\n",
       "\n",
       "* ** [geohash_tags](data/geohash_tags.csv)**. Points converted to counts of tags per geohash\n",
       "* ** [residential_roads](data/residential_roads.csv)**. Residential roads per 4 digit geohash\n",
       "* ** [nonres_roads](data/nonres_roads.csv)**. Non residential roads per 4 digit geohash\n",
       "\n",
       "## References\n",
       "<ul><li> <strong>us_geohashes</string>, <em>metapack+http://library.metatab.org/civicknowledge.com-geohash-us.csv#us_geohashes</em>. All 4 digit geohases in the continential US</li><li> <strong><a href=\"https://download.geofabrik.de/north-america-latest.osm.pbf\">north-america-latest</a></strong>. OSM North America extract</li><li> <strong>points</string>, <em>data/csv/points.csv</em>. Points from the OSM file</li><li> <strong>lines</string>, <em>data/csv/lines.csv</em>. Lines from the OSM file</li><li> <strong>multipolygons</string>, <em>data/csv/multipolygons.csv</em>. Polygons from the OSM file</li><li> <strong>multilinestrings</string>, <em>data/csv/multilinestrings.csv</em>. Lines from the OSM file</li><li> <strong>other_relations</string>, <em>data/csv/other_relations.csv</em>. Other geo data from the OSM file</li><ul>\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "from pathlib import Path\n",
    "import metapack as mp\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from auto_tqdm import tqdm \n",
    "from shapely.wkt import loads\n",
    "from shapely.geometry import Polygon\n",
    "import libgeohash as gh \n",
    "from operator import mul\n",
    "import numpy as np\n",
    "\n",
    "doc = mp.jupyter.open_source_package()\n",
    "doc.set_sys_path()\n",
    "import pylib\n",
    "\n",
    "ea_epsg = 2163 #US Equal Area projection\n",
    "\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "\n",
    "from pylib import lines_logger, points_logger\n",
    "lines_logger.setLevel(logging.DEBUG)\n",
    "points_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "pkg_root = Path(doc.path).parent\n",
    "pkg = mp.open_package(pkg_root)\n",
    "pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "precise-swaziland",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = pylib.open_cache(pkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "furnished-indonesia",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_h = mp.open_package('http://library.metatab.org/civicknowledge.com-geohash-us.csv')\n",
    "hashes = pkg_h.resource('us_geohashes').geoframe().rename(columns={'geohash':'gh4'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "warming-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df = cache.get_df('points/tags_df')\n",
    "tags_df['gh4'] = tags_df.geohash.str.slice(0,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pressing-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a reduced collection of points\n",
    "\n",
    "bus_tags = ['restaurant','fast_food','cafe','convenience','bank','supermarket']\n",
    "low_dens_tags = ['grave_yard','camp_site','camp_pitch','bench','view_point']\n",
    "\n",
    "def collect_densities(tags):\n",
    "    t = tags_df.set_index(['geohash'])[['amenity','shop','tourism']].stack().to_frame().reset_index()\n",
    "    t.columns = ['geohash','group','type']\n",
    "    t = t[t.type.isin(tags)]\n",
    "    #t.to_csv('tags.csv')\n",
    "    t[['gh4']] = t.geohash.str.slice(0,4)\n",
    "    t = t.merge(tags_df[['geohash', 'geometry']])\n",
    "\n",
    "    t['dummy'] = 1\n",
    "    t = hashes[['gh4']].merge(t)# only in the continential US\n",
    "    t = t.groupby(t.geohash.str.slice(0,6)).dummy.count().to_frame().reset_index()\n",
    "    t['geometry'] = t.geohash.apply(lambda ghc: Polygon([ele[::-1] for ele in gh.bbox(ghc, coordinates = True)]))\n",
    "    t = gpd.GeoDataFrame(t, crs=4326).to_crs(3857) # Web Mercator\n",
    "    t = t[t.dummy >2]\n",
    "\n",
    "    t['a'] = t.geohash.apply(lambda v: (mul(*gh.dimensions(v, True))))\n",
    "    t['density'] = (t.dummy/t.a)*1e6\n",
    "    t['dummy'] = t.dummy.clip(0,t.dummy.mean()*5)\n",
    "    return t\n",
    "\n",
    "bus_densities =  collect_densities(bus_tags)\n",
    "ld_densities =  collect_densities(low_dens_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "reported-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_densities =  bus_densities[['geohash','dummy','density']].rename(columns={'dummy':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "effective-going",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash</th>\n",
       "      <th>count</th>\n",
       "      <th>density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9mgvem</td>\n",
       "      <td>11.0</td>\n",
       "      <td>17.645323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9mgveq</td>\n",
       "      <td>17.0</td>\n",
       "      <td>27.271765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9mgzcv</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.445905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9mgzcx</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.893461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9mgzum</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.280334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   geohash  count    density\n",
       "0   9mgvem   11.0  17.645323\n",
       "1   9mgveq   17.0  27.271765\n",
       "5   9mgzcv    4.0   6.445905\n",
       "6   9mgzcx    8.0  12.893461\n",
       "14  9mgzum    7.0  11.280334"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_densities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "consistent-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the points to geohashes\n",
    "\n",
    "if False:\n",
    "    import contextily as ctx\n",
    "\n",
    "    t = ld_densities.copy()\n",
    "    t['geometry'] = t.buffer(10000)\n",
    "    ax =t.plot(column='density', scheme='fisher_jenks_sampled',figsize=(10, 8))\n",
    "    ctx.add_basemap(ax, source=ctx.providers.Stamen.Toner)\n",
    "    display(t.dummy.describe())\n",
    "   \n",
    "if False:\n",
    "    t = gpd.sjoin( hd_hashes[['geohash', 'geometry', 'density']], places.to_crs(3857))\n",
    "    t[['stusab']] = t.statefp.apply(lambda v: stusab.get(int(v)))\n",
    "    place_den = t[['namelsad', 'stusab', 'geohash','density', 'geometry']]\n",
    "    place_den = place_den.groupby('geohash').first().sort_values('density', ascending=False)\n",
    "    place_den.to_csv('densities.csv')\n",
    "\n",
    "if False and not Path('places.csv').exists():\n",
    "    import rowgenerators as rg\n",
    "    from geoid.censusnames import stusab\n",
    "    cstates =  [ st for st in stusab.values() if st not in ['HI', 'AK', 'PR', 'VI', 'MP', 'GU', 'AS'] ]\n",
    "    frames = [rg.geoframe(\"censusgeo://2018/5/{}/place\".format(st)) for st in tqdm(cstates)]\n",
    "    places = pd.concat(frames)\n",
    "    places.to_csv('places.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-enhancement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
