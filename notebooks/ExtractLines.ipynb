{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "import libgeohash as gh\n",
    "import matplotlib.pyplot as plt\n",
    "import metapack as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rowgenerators as rg\n",
    "import seaborn as sns\n",
    "import utm\n",
    "from IPython.display import display \n",
    "from demosearch import FileCache\n",
    "from demosearch.util import munge_pbar,  run_mp, gh_path, disaggregate\n",
    "from geoid.censusnames import stusab\n",
    "from itertools import chain \n",
    "import shapely\n",
    "from shapely.geometry import LineString, Point, Polygon\n",
    "from shapely.wkt import loads as loads_wkt\n",
    "from itertools import chain \n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context('notebook')\n",
    "mp.jupyter.init()\n",
    "\n",
    "utm_crs = 26911\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Open Street Map Extract for Demographic Search</h1>\n",
       "<p><code>civicknowledge.com-osm-demosearch-1.1.1</code> Last Update: 2021-01-31T04:28:56</p>\n",
       "<p>__</p>\n",
       "<h2>Contacts</h2>\n",
       "<ul>\n",
       "<li><strong>Wrangler</strong> <a href=\"mailto:eric@civicknowledge.com\">Eric Busboom</a>, <a href=\"http://civicknowledge.com\">Civic Knowledge</a></li>\n",
       "</ul>\n",
       "<h2>Resources</h2>\n",
       "<ul>\n",
       "<li><strong> <a href=\"notebooks/ExtractPoints.ipynb#geohash_tags\">geohash_tags</a></strong>. Points converted to counts of tags per geohash</li>\n",
       "</ul>\n",
       "<h2>References</h2>\n",
       "<ul><li> <strong>us_geohashes</string>, <em>index:civicknowledge.com-geohash-us#us_geohashes</em>. All 4 digit geohases in the continential US</li><li> <strong><a href=\"https://download.geofabrik.de/north-america-latest.osm.pbf\">north-america-latest</a></strong>. OSM North America extract</li><li> <strong>points</string>, <em>data/csv/points.csv</em>. Points from the OSM file</li><li> <strong>lines</string>, <em>data/csv/lines.csv</em>. Lines from the OSM file</li><li> <strong>multipolygons</string>, <em>data/csv/multipolygons.csv</em>. Polygons from the OSM file</li><li> <strong>multilinestrings</string>, <em>data/csv/multilinestrings.csv</em>. Lines from the OSM file</li><li> <strong>other_relations</string>, <em>data/csv/other_relations.csv</em>. Other geo data from the OSM file</li><ul>"
      ],
      "text/plain": [
       "# Open Street Map Extract for Demographic Search\n",
       "`civicknowledge.com-osm-demosearch-1.1.1` Last Update: 2021-01-31T04:28:56\n",
       "\n",
       "__\n",
       "\n",
       "\n",
       "\n",
       "\n",
       " \n",
       "\n",
       "## Contacts\n",
       "\n",
       "* **Wrangler** [Eric Busboom](mailto:eric@civicknowledge.com), [Civic Knowledge](http://civicknowledge.com)\n",
       "\n",
       "## Resources\n",
       "\n",
       "* ** [geohash_tags](notebooks/ExtractPoints.ipynb#geohash_tags)**. Points converted to counts of tags per geohash\n",
       "\n",
       "## References\n",
       "<ul><li> <strong>us_geohashes</string>, <em>index:civicknowledge.com-geohash-us#us_geohashes</em>. All 4 digit geohases in the continential US</li><li> <strong><a href=\"https://download.geofabrik.de/north-america-latest.osm.pbf\">north-america-latest</a></strong>. OSM North America extract</li><li> <strong>points</string>, <em>data/csv/points.csv</em>. Points from the OSM file</li><li> <strong>lines</string>, <em>data/csv/lines.csv</em>. Lines from the OSM file</li><li> <strong>multipolygons</string>, <em>data/csv/multipolygons.csv</em>. Polygons from the OSM file</li><li> <strong>multilinestrings</string>, <em>data/csv/multilinestrings.csv</em>. Lines from the OSM file</li><li> <strong>other_relations</string>, <em>data/csv/other_relations.csv</em>. Other geo data from the OSM file</li><ul>\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pkg = mp.jupyter.open_package()\n",
    "pkg = mp.jupyter.open_source_package()\n",
    "pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hashes = pkg.reference('us_geohashes').geoframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5306 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 53s, sys: 31 s, total: 3min 24s\n",
      "Wall time: 3min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pathlib import Path\n",
    "fp = pkg.reference('lines').resolved_url.fspath\n",
    "op = Path.cwd().parent.joinpath('data','cache', 'lines')\n",
    "if not op.exists():\n",
    "    op.mkdir(parents=True)\n",
    "op\n",
    "\n",
    "#\n",
    "# Write out the lines files into chunks so we can run it in multiple\n",
    "# processes\n",
    "frames = []\n",
    "chunksize = 10000\n",
    "total = int(53065618/ chunksize)\n",
    "with pd.read_csv(fp, chunksize=chunksize, low_memory=False) as reader:\n",
    "    for i, df in tqdm(enumerate(reader), total=total):\n",
    "        p = op.joinpath(f\"{i}.feather\")\n",
    "        if not op.exists():\n",
    "            df.reset_index().to_feather(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_type = {\n",
    "    'residential': 'r',\n",
    "    'primary':'1',\n",
    "    'secondary':'2',\n",
    "    'tertiary':'3',\n",
    "    'motorway':'m',\n",
    "    'motorway_link ':'l',\n",
    "    'trunk':'t'\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split By Geohash:   0%|          | 0/531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 722 ms, sys: 230 ms, total: 953 ms\n",
      "Wall time: 13min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Process each of the seperate files, then\n",
    "# write them back out for later recombination\n",
    "\n",
    "cache = FileCache(Path.cwd().parent.joinpath('data','cache'))\n",
    "tasks = [ [e] for e in list(op.glob(\"**/*.feather\"))]\n",
    "    \n",
    "def _f(fn):\n",
    "\n",
    "    t = pd.read_feather(fn)\n",
    "    t = t[t.highway.isin(list(hw_type.keys()))]\n",
    "    t['highway'] = t.highway.replace(hw_type) # Cuts file size by 100M\n",
    "    t['geometry'] = t.geometry.apply(shapely.wkt.loads)\n",
    "    gdf = gpd.GeoDataFrame(t, crs=4326)\n",
    "    t = gpd.overlay(gdf, hashes)\n",
    "    \n",
    "    t = t[['osm_id','geohash','utm_epsg','utm_area','highway','geometry']]\n",
    "    \n",
    "    key = f\"recombine/{fn.stem}\"\n",
    "\n",
    "    cache.put_df(key, t)\n",
    "    \n",
    "    return key\n",
    "  \n",
    "recombine_keys = run_mp(_f, tasks, desc='Split By Geohash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split By Geohash:   0%|          | 0/531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 408 ms, sys: 134 ms, total: 542 ms\n",
      "Wall time: 4min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "def _f(key):\n",
    "    df  = cache.get_df(key)\n",
    "    okeys = []\n",
    "    errs = []\n",
    "    for idx, g in df.groupby('utm_epsg'):\n",
    "        _, fn = key.split('/')\n",
    "        okey = f'epsg/{idx}/{fn}'\n",
    "        \n",
    "        try:\n",
    "             \n",
    "            geometry = g.to_crs(epsg=idx).geometry\\\n",
    "                            .simplify(20, False)\\\n",
    "                            .apply( lambda e: shapely.wkt.dumps(e, rounding_precision=0) )\n",
    "\n",
    "            g = pd.DataFrame(g).assign(geometry=geometry)\n",
    "            \n",
    "            cache.put_df(okey, g)\n",
    "            okeys.append(okey)\n",
    "        except AttributeError:\n",
    "            print(f'!!! Error converting {idx} in {key}')\n",
    "            errs.append((key,okey))\n",
    "    return okeys\n",
    "\n",
    "epsg_keys = run_mp(_f, [ (e,) for e in recombine_keys], desc='Split By Geohash') \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.87 s, sys: 1.44 s, total: 7.31 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ek = list(chain(*epsg_keys))\n",
    "frames = [cache.get_df(e) for e in tqdm(ek)]\n",
    "t = pd.concat(frames)\n",
    "residential = t[t.highway == 'r']\n",
    "nonres = t[t.highway != 'r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%time residential.to_csv('../data/residential.csv')\n",
    "%time nonres.to_csv('../data/nonres.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
